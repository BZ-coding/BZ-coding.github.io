<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>大模型检索增强生成RAG - 张胜东的博客</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="张胜东"><meta name=description content="背景 最近LLM大模型异常火热，我判断RAG检索增强是未来的一个重要切入点，所以想试试做个demo，走一遍流程。 主要的想法是利用我的微信公众号"><meta name=keywords content="rag,llm,langchain,faiss"><meta name=baidu-site-verification content="qWR9jJPJ9e"><meta name=google-site-verification content="s9FkJZw4X2alyC8-nsdZgiPHBwX6uqr1QVNxRaGfDKY"><meta name=generator content="Hugo 0.102.3 with theme even"><link rel=canonical href=https://www.zhangshengdong.com/post/rag/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="大模型检索增强生成RAG"><meta property="og:description" content="背景 最近LLM大模型异常火热，我判断RAG检索增强是未来的一个重要切入点，所以想试试做个demo，走一遍流程。 主要的想法是利用我的微信公众号"><meta property="og:type" content="article"><meta property="og:url" content="https://www.zhangshengdong.com/post/rag/"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-04-06T01:41:58+08:00"><meta property="article:modified_time" content="2024-06-12T01:48:31+08:00"><meta itemprop=name content="大模型检索增强生成RAG"><meta itemprop=description content="背景 最近LLM大模型异常火热，我判断RAG检索增强是未来的一个重要切入点，所以想试试做个demo，走一遍流程。 主要的想法是利用我的微信公众号"><meta itemprop=datePublished content="2024-04-06T01:41:58+08:00"><meta itemprop=dateModified content="2024-06-12T01:48:31+08:00"><meta itemprop=wordCount content="3126"><meta itemprop=keywords content="rag,llm,langchain,faiss,"><meta name=twitter:card content="summary"><meta name=twitter:title content="大模型检索增强生成RAG"><meta name=twitter:description content="背景 最近LLM大模型异常火热，我判断RAG检索增强是未来的一个重要切入点，所以想试试做个demo，走一遍流程。 主要的想法是利用我的微信公众号"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>张胜东的博客</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>张胜东的博客</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>大模型检索增强生成RAG</h1><div class=post-meta><span class=post-time>2024-04-06</span><div class=post-category><a href=/categories/%E5%B7%A5%E7%A8%8B/>工程</a></div><span class=more-meta>约 3126 字</span>
<span class=more-meta>预计阅读 7 分钟</span>
<span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> 次阅读</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#背景>背景</a></li><li><a href=#rag>RAG</a><ul><li><a href=#读取本地embedding模型>读取本地embedding模型</a></li><li><a href=#文本embedding化>文本embedding化</a></li><li><a href=#构建向量引擎>构建向量引擎</a></li><li><a href=#创建对话模型>创建对话模型</a></li><li><a href=#构建流水线>构建流水线</a></li><li><a href=#效果>效果</a></li></ul></li></ul></li></ul></nav></div></div><div class=post-content><h2 id=背景>背景</h2><p>最近LLM大模型异常火热，我判断RAG检索增强是未来的一个重要切入点，所以想试试做个demo，走一遍流程。</p><p>主要的想法是利用我的微信公众号或利用gradio搭建网页，部署对话查询服务。LLM-EM将文献分段向量化存入向量数据库，当query来后，将query的embedding拿到向量数据库中检索出最相近的一条或几条，拼到prompt中喂给LLM-CHAT进行回答，并将检索结果以markdown引用形式拼在回答后面。</p><p>demo仓库在这里：<a href=https://github.com/BZ-coding/rag>https://github.com/BZ-coding/rag</a></p><h2 id=rag>RAG</h2><h3 id=读取本地embedding模型>读取本地embedding模型</h3><p>不得不说，一开始我为了图省事，直接把为对话模型准备的chinese-llama-7b，当作embedding模型读了进来。结果就是query到的相关语句没啥相关性。</p><p>可换成智谱的bge-large-zh模型后，效果一下就是质的飞跃，不得不感叹：“智谱牛逼”。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.embeddings</span> <span class=kn>import</span> <span class=n>HuggingFaceInstructEmbeddings</span><span class=p>,</span> <span class=n>HuggingFaceBgeEmbeddings</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_path</span> <span class=o>=</span> <span class=s2>&#34;/mnt/nfs/zsd_server/models/huggingface/embedding_models/BAAI/bge-large-zh-v1.5/&#34;</span>
</span></span><span class=line><span class=cl><span class=n>model_kwargs</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;device&#39;</span><span class=p>:</span> <span class=s1>&#39;cuda&#39;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>encode_kwargs</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;normalize_embeddings&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>}</span> <span class=c1># set True to compute cosine similarity</span>
</span></span><span class=line><span class=cl><span class=n>embedding_model</span> <span class=o>=</span> <span class=n>HuggingFaceBgeEmbeddings</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=o>=</span><span class=n>model_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>model_kwargs</span><span class=o>=</span><span class=n>model_kwargs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>encode_kwargs</span><span class=o>=</span><span class=n>encode_kwargs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>query_instruction</span><span class=o>=</span><span class=s2>&#34;为这个句子生成表示以用于检索相关文章：&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>embedding_model</span><span class=o>.</span><span class=n>query_instruction</span> <span class=o>=</span> <span class=s2>&#34;为这个句子生成表示以用于检索相关文章：&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>embedding_model</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>HuggingFaceBgeEmbeddings(client=SentenceTransformer(
</span></span><span class=line><span class=cl>  (0): Transformer({&#39;max_seq_length&#39;: 512, &#39;do_lower_case&#39;: True}) with Transformer model: BertModel 
</span></span><span class=line><span class=cl>  (1): Pooling({&#39;word_embedding_dimension&#39;: 1024, &#39;pooling_mode_cls_token&#39;: True, &#39;pooling_mode_mean_tokens&#39;: False, &#39;pooling_mode_max_tokens&#39;: False, &#39;pooling_mode_mean_sqrt_len_tokens&#39;: False, &#39;pooling_mode_weightedmean_tokens&#39;: False, &#39;pooling_mode_lasttoken&#39;: False, &#39;include_prompt&#39;: True})
</span></span><span class=line><span class=cl>  (2): Normalize()
</span></span><span class=line><span class=cl>), model_name=&#39;/mnt/nfs/zsd_server/models/huggingface/embedding_models/BAAI/bge-large-zh-v1.5/&#39;, cache_folder=None, model_kwargs={&#39;device&#39;: &#39;cuda&#39;}, encode_kwargs={&#39;normalize_embeddings&#39;: True}, query_instruction=&#39;为这个句子生成表示以用于检索相关文章：&#39;, embed_instruction=&#39;&#39;)
</span></span></code></pre></td></tr></table></div></div><h3 id=文本embedding化>文本embedding化</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;刑法.txt&#34;</span><span class=p>,</span> <span class=s2>&#34;r&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>readlines</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>[</span><span class=n>d</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span> <span class=k>for</span> <span class=n>d</span> <span class=ow>in</span> <span class=n>data</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data_embeddings</span> <span class=o>=</span> <span class=n>embedding_model</span><span class=o>.</span><span class=n>embed_documents</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text_embedding_pairs</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>data_embeddings</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>这步没啥说的，反正用的都是langchain的接口。</p><p>保存我是直接把最费时的embedding结果给保存了。感觉其实可以把后面的向量数据库保存的，但没细纠，以后可以研究研究。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pickle</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pickle</span><span class=o>.</span><span class=n>dump</span><span class=p>(</span><span class=n>text_embedding_pairs</span><span class=p>,</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;text_embedding_pairs_BAAI.pkl&#39;</span><span class=p>,</span> <span class=s1>&#39;wb&#39;</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=构建向量引擎>构建向量引擎</h3><p>向量引擎我用的是faiss。当然，你也可以用别的，我只是觉得faiss的名气比较大而已。而且，记得我看过一篇文章，说是就现在向量数据的规模，其实根本用不上重型引擎，写个map都行。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain_community.vectorstores</span> <span class=kn>import</span> <span class=n>FAISS</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>faiss</span> <span class=o>=</span> <span class=n>FAISS</span><span class=o>.</span><span class=n>from_embeddings</span><span class=p>(</span><span class=n>text_embedding_pairs</span><span class=p>,</span> <span class=n>embedding_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>retriever</span> <span class=o>=</span> <span class=n>faiss</span><span class=o>.</span><span class=n>as_retriever</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=创建对话模型>创建对话模型</h3><h4 id=prompt>prompt</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.prompts</span> <span class=kn>import</span> <span class=n>ChatPromptTemplate</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>template</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;你是问答任务助手。使用以下检索到的上下文片段来回答问题。如果你不知道答案，就说你不知道。最多使用三个句子，保持答案简洁。
</span></span></span><span class=line><span class=cl><span class=s2>Question: </span><span class=si>{question}</span><span class=s2> 
</span></span></span><span class=line><span class=cl><span class=s2>Context: </span><span class=si>{context}</span><span class=s2> 
</span></span></span><span class=line><span class=cl><span class=s2>Answer:
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=n>ChatPromptTemplate</span><span class=o>.</span><span class=n>from_template</span><span class=p>(</span><span class=n>template</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>prompt</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>input_variables=[&#39;context&#39;, &#39;question&#39;] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#39;context&#39;, &#39;question&#39;], template=&#39;你是问答任务助手。使用以下检索到的上下文片段来回答问题。如果你不知道答案，就说你不知道。最多使用三个句子，保持答案简洁。\nQuestion: {question} \nContext: {context} \nAnswer:\n&#39;))]
</span></span></code></pre></td></tr></table></div></div><h4 id=对话模型>对话模型</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>LlamaTokenizer</span><span class=p>,</span> <span class=n>LlamaForCausalLM</span><span class=p>,</span> <span class=n>GenerationConfig</span><span class=p>,</span> <span class=n>pipeline</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.llms</span> <span class=kn>import</span> <span class=n>HuggingFacePipeline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>base_model_path</span> <span class=o>=</span> <span class=s2>&#34;/mnt/nfs/zsd_server/models/huggingface/chinese-alpaca-2-7b/&#34;</span>
</span></span><span class=line><span class=cl><span class=n>base_model</span> <span class=o>=</span> <span class=n>LlamaForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>base_model_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=c1># load_in_8bit=True,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>LlamaTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>base_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pipe</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;text-generation&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>base_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_length</span><span class=o>=</span><span class=mi>4096</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>temperature</span><span class=o>=</span><span class=mf>0.6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_p</span><span class=o>=</span><span class=mf>0.95</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>repetition_penalty</span><span class=o>=</span><span class=mf>1.2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>local_llm</span> <span class=o>=</span> <span class=n>HuggingFacePipeline</span><span class=p>(</span><span class=n>pipeline</span><span class=o>=</span><span class=n>pipe</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=构建流水线>构建流水线</h3><p>虽然以上我们已经集齐了所需的全部零件，但要把rag跑起来，还需要写一个完整的调用流程。虽然这个流程我们自己写也行，但毕竟langchain已经提供了，不用白不用。不过话说回来，这也是langchain饱受诟病的地方，被认为封装过度了。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.schema.runnable</span> <span class=kn>import</span> <span class=n>RunnablePassthrough</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.schema.output_parser</span> <span class=kn>import</span> <span class=n>StrOutputParser</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>rag_chain</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;context&#34;</span><span class=p>:</span> <span class=n>retriever</span><span class=p>,</span>  <span class=s2>&#34;question&#34;</span><span class=p>:</span> <span class=n>RunnablePassthrough</span><span class=p>()}</span> 
</span></span><span class=line><span class=cl>    <span class=o>|</span> <span class=n>prompt</span> 
</span></span><span class=line><span class=cl>    <span class=o>|</span> <span class=n>local_llm</span>
</span></span><span class=line><span class=cl>    <span class=o>|</span> <span class=n>StrOutputParser</span><span class=p>()</span> 
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>rag_chain</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>{
</span></span><span class=line><span class=cl>  context: VectorStoreRetriever(tags=[&#39;FAISS&#39;, &#39;HuggingFaceBgeEmbeddings&#39;], vectorstore=&lt;langchain_community.vectorstores.faiss.FAISS object at 0x7aa65a471e90&gt;),
</span></span><span class=line><span class=cl>  question: RunnablePassthrough()
</span></span><span class=line><span class=cl>}
</span></span><span class=line><span class=cl>| ChatPromptTemplate(input_variables=[&#39;context&#39;, &#39;question&#39;], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#39;context&#39;, &#39;question&#39;], template=&#39;你是问答任务助手。使用以下检索到的上下文片段来回答问题。如果你不知道答案，就说你不知道。最多使用三个句子，保持答案简洁。\nQuestion: {question} \nContext: {context} \nAnswer:\n&#39;))])
</span></span><span class=line><span class=cl>| HuggingFacePipeline(pipeline=&lt;transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7aa5b00b5d50&gt;)
</span></span><span class=line><span class=cl>| StrOutputParser()
</span></span></code></pre></td></tr></table></div></div><h3 id=效果>效果</h3><p>我们先来看看对于同一个问题，rag的回答：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;持有管制刀具怎么判？&#34;</span>
</span></span><span class=line><span class=cl><span class=n>rag_chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>&#34;Human: 你是问答任务助手。使用以下检索到的上下文片段来回答问题。如果你不知道答案，就说你不知道。最多使用三个句子，保持答案简洁。\nQuestion: 持有管制刀具怎么判？ \nContext: [Document(page_content=&#39;第二百九十七条 违反法律规定，携带武器、管制刀具或者爆炸物参加集会、游行、示威的，处三年以下有期徒刑、拘役、管制或者剥夺政治权利。&#39;), Document(page_content=&#39;第一百三十条 非法携带枪支、弹药、管制刀具或者爆炸性、易燃性、放射性、毒害性、腐蚀性物品，进入公共场所或者公共交通工具，危及公共安全，情节严重的，处三年以下有期徒刑、拘役或者管制。&#39;), Document(page_content=&#39;第一百二十八条 违反枪支管理规定，非法持有、私藏枪支、弹药的，处三年以下有期徒刑、拘役或者管制；情节严重的，处三年以上七年以下有期徒刑。依法配备公务用枪的人员，非法出租、出借枪支的，依照前款的规定处罚。依法配置枪支的人员，非法出租、出借枪支，造成严重后果的，依照第一款的规定处罚。单位犯第二款、第三款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照第一款的规定处罚。&#39;), Document(page_content=&#39;第一百二十条之六 明知是宣扬恐怖主义、极端主义的图书、音频视频资料或者其他物品而非法持有，情节严重的，处三年以下有期徒刑、拘役或者管制，并处或者单处罚金。&#39;)] \nAnswer:\n根据中国刑法相关规定,持械参与集会等行为属于违法行为.如果情节较轻会被罚款或拘留,如情节较为恶劣则可能被判处3年以下有期徒刑、拘役或者管制;而非法持有枪支和炸弹的行为将被视为犯罪并且将会受到更严厉的惩罚.(参见第102条)&#34;
</span></span></code></pre></td></tr></table></div></div><blockquote><p>根据中国刑法相关规定,持械参与集会等行为属于违法行为.如果情节较轻会被罚款或拘留,如情节较为恶劣则可能被判处3年以下有期徒刑、拘役或者管制;而非法持有枪支和炸弹的行为将被视为犯罪并且将会受到更严厉的惩罚.(参见第102条)</p></blockquote><p>而如果我们直接把query去问对话模型，由于没有了匹配的资料，大模型就开始啰嗦+幻觉了：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>local_llm</span><span class=p>(</span><span class=n>query</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>&#39;持有管制刀具怎么判？第159条：禁止携带、运输危险物品，违者处五年以下有期徒刑或者拘役；情节严重的，处以五年以上十年以下有期徒刑。这是指在公共场所非法使用或私藏枪支弹药等具有杀伤力的工具的行为属于刑法规定中的危害社会罪行为之一——暴力犯罪（第三百零七条）和聚众斗殴罪行（第二百九十条第一款）两个罪名中任意一个构成要件的规定情况下的情形下才可能被认定为&#34;持枪抢劫案&#34;.而如果仅仅是因携带了一把匕首或是其它类似性质的小型武器就单独定性于“持刀伤害”一类的刑事案件当中.\n总之，根据我国现行法律制度以及司法解释实施细则所确立的标准来看的话，只要当事人依法依规地进行了合法登记并遵守相关法规要求的情况下，他/她就不会因为自己拥有一支步枪就被判定犯下了“持枪抢劫案”这样的重罪犯事件&#39;
</span></span></code></pre></td></tr></table></div></div><blockquote><p>第159条：禁止携带、运输危险物品，违者处五年以下有期徒刑或者拘役；情节严重的，处以五年以上十年以下有期徒刑。这是指在公共场所非法使用或私藏枪支弹药等具有杀伤力的工具的行为属于刑法规定中的危害社会罪行为之一——暴力犯罪（第三百零七条）和聚众斗殴罪行（第二百九十条第一款）两个罪名中任意一个构成要件的规定情况下的情形下才可能被认定为"持枪抢劫案".而如果仅仅是因携带了一把匕首或是其它类似性质的小型武器就单独定性于“持刀伤害”一类的刑事案件当中.\n总之，根据我国现行法律制度以及司法解释实施细则所确立的标准来看的话，只要当事人依法依规地进行了合法登记并遵守相关法规要求的情况下，他/她就不会因为自己拥有一支步枪就被判定犯下了“持枪抢劫案”这样的重罪犯事件</p></blockquote><p>对比可看出，还是rag的答案更好些，起码不会一本正经的胡说八道了。</p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/images/money_weixin_20200719212002.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/images/alipay_20200801211208.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags><a href=/tags/rag/>rag</a>
<a href=/tags/llm/>llm</a>
<a href=/tags/langchain/>langchain</a>
<a href=/tags/faiss/>faiss</a></div><nav class=post-nav><a class=next href=/post/wordpress/><span class="next-text nav-default">使用sqlite搭建wordpress博客</span>
<span class="next-text nav-mobile">下一篇</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js crossorigin=anonymous></script>
<script type=text/javascript>var gitalk=new Gitalk({id:"2024-04-06 01:41:58 \u002b0800 \u002b0800",title:"大模型检索增强生成RAG",clientID:"5ea75f603117948d8d37",clientSecret:"26c617c6bce9a975c2a65a68f1ca2a2cc7dde587",repo:"blog",owner:"zhangsheng377",admin:["zhangsheng377"],body:decodeURI(location.href),proxy:"https://vercel.prohibitorum.top/github_access_token"});gitalk.render("gitalk-container")</script><noscript>Please enable JavaScript to view the <a href=https://github.com/gitalk/gitalk>comments powered by gitalk.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:435878393@qq.com class="iconfont icon-email" title=email></a>
<a href=https://www.linkedin.com/in/zhangshengdong/ class="iconfont icon-linkedin" title=linkedin></a>
<a href=https://github.com/zhangsheng377/ class="iconfont icon-github" title=github></a>
<a href=https://www.zhangshengdong.com/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>由 <a class=hexo-link href=https://gohugo.io>Hugo</a> 强力驱动</span>
<span class=division>|</span>
<span class=theme-info>主题 -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>本站总访问量 <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> 次</span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv>本站总访客数 <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> 人</span></div><span class=copyright-year>&copy;
2019 -
2024<span class=heart><i class="iconfont icon-heart"></i></span><span><a href=https://beian.miit.gov.cn/ target=_blank>苏ICP备15009593号-1</a></span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/raphael@2.2.7/raphael.min.js integrity="sha256-67By+NpOtm9ka1R6xpUefeGOY8kWWHHRAKlvaTJ7ONI=" crossorigin=anonymous></script> <script src=https://cdn.jsdelivr.net/npm/flowchart.js@1.8.0/release/flowchart.min.js integrity="sha256-zNGWjubXoY6rb5MnmpBNefO0RgoVYfle9p0tvOQM+6k=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.js integrity="sha256-4O4pS1SH31ZqrSO2A/2QJTVjTPqVe+jnYgOWUVr7EEc=" crossorigin=anonymous></script> <script src=https://cdn.jsdelivr.net/npm/snapsvg@0.5.1/dist/snap.svg-min.js integrity="sha256-oI+elz+sIm+jpn8F/qEspKoKveTc5uKeFHNNVexe6d8=" crossorigin=anonymous></script> <script src=https://cdn.jsdelivr.net/npm/underscore@1.8.3/underscore-min.js integrity="sha256-obZACiHd7gkOk9iIL/pimWMTJ4W/pBsKu+oZnSeBIek=" crossorigin=anonymous></script> <script src=https://cdn.jsdelivr.net/gh/bramp/js-sequence-diagrams@2.0.1/dist/sequence-diagram-min.js integrity=sha384-8748Vn52gHJYJI0XEuPB2QlPVNUkJlJn9tHqKec6J3q2r9l8fvRxrgn/E5ZHV0sP crossorigin=anonymous></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/bramp/js-sequence-diagrams@2.0.1/dist/sequence-diagram-min.css integrity=sha384-6QbLKJMz5dS3adWSeINZe74uSydBGFbnzaAYmp+tKyq60S7H2p6V7g1TysM5lAaF crossorigin=anonymous><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
<script id=baidu_analytics>var _hmt=_hmt||[];(function(){if(window.location.hostname==="localhost")return;var t,e=document.createElement("script");e.async=!0,e.src="https://hm.baidu.com/hm.js?c602db2501643f661d9789f9e9707386",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>