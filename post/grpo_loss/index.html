<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>GRPO Loss初期为0的原因与改进方法 - 张胜东的博客</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="张胜东"><meta name=description content="引言 在家里自己用OpenR1准备从qwen-base训出个R1模型来，结果跑了demo数据，发现前100多步的loss几乎都是0： 在搜索相关"><meta name=keywords content="ai,llm,grpo,loss"><meta name=baidu-site-verification content="qWR9jJPJ9e"><meta name=google-site-verification content="s9FkJZw4X2alyC8-nsdZgiPHBwX6uqr1QVNxRaGfDKY"><meta name=generator content="Hugo 0.102.3 with theme even"><link rel=canonical href=https://www.zhangshengdong.com/post/grpo_loss/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="GRPO Loss初期为0的原因与改进方法"><meta property="og:description" content="引言 在家里自己用OpenR1准备从qwen-base训出个R1模型来，结果跑了demo数据，发现前100多步的loss几乎都是0： 在搜索相关"><meta property="og:type" content="article"><meta property="og:url" content="https://www.zhangshengdong.com/post/grpo_loss/"><meta property="article:section" content="post"><meta property="article:published_time" content="2025-02-11T23:54:58+08:00"><meta property="article:modified_time" content="2025-03-06T00:28:01+08:00"><meta itemprop=name content="GRPO Loss初期为0的原因与改进方法"><meta itemprop=description content="引言 在家里自己用OpenR1准备从qwen-base训出个R1模型来，结果跑了demo数据，发现前100多步的loss几乎都是0： 在搜索相关"><meta itemprop=datePublished content="2025-02-11T23:54:58+08:00"><meta itemprop=dateModified content="2025-03-06T00:28:01+08:00"><meta itemprop=wordCount content="2045"><meta itemprop=keywords content="ai,llm,grpo,loss,"><meta name=twitter:card content="summary"><meta name=twitter:title content="GRPO Loss初期为0的原因与改进方法"><meta name=twitter:description content="引言 在家里自己用OpenR1准备从qwen-base训出个R1模型来，结果跑了demo数据，发现前100多步的loss几乎都是0： 在搜索相关"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>张胜东的博客</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>张胜东的博客</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>GRPO Loss初期为0的原因与改进方法</h1><div class=post-meta><span class=post-time>2025-02-11</span><div class=post-category><a href=/categories/%E7%AE%97%E6%B3%95/>算法</a></div><span class=more-meta>约 2045 字</span>
<span class=more-meta>预计阅读 5 分钟</span>
<span id=busuanzi_container_page_pv class=more-meta><span id=busuanzi_value_page_pv><img src=/img/spinner.svg alt=spinner.svg></span> 次阅读</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#引言>引言</a></li><li><a href=#grpo-loss函数分析>GRPO Loss函数分析</a></li><li><a href=#初始loss为零的原因>初始Loss为零的原因</a><ul><li><a href=#第一步的kl为什么是0>第一步的KL为什么是0？</a></li><li><a href=#第一步的advantages为什么是0>第一步的advantages为什么是0？</a></li></ul></li><li><a href=#有啥改进措施加速方法>有啥改进措施（加速方法）？</a><ul><li><a href=#loss是0为啥还能训>loss是0为啥还能训？</a></li><li><a href=#改进措施>改进措施</a></li></ul></li></ul></li></ul></nav></div></div><div class=post-content><h2 id=引言>引言</h2><p>在家里自己用OpenR1准备从qwen-base训出个R1模型来，结果跑了demo数据，发现前100多步的loss几乎都是0：</p><p><img src=/images/grpo_train.png alt=grpo训练记录></p><p>在搜索相关资料时，发现Hugging Face的TRL库中也有类似的问题讨论：</p><blockquote><p><a href=https://github.com/huggingface/trl/issues/2703#issuecomment-2625274839>GRPO: 为什么损失在前K步为零，然后随着时间的推移增加？</a></p></blockquote><p>这表明，GRPO训练初期损失为零可能是一个固有现象。</p><p>至于我的loss为啥一直是0，这应该与我的lr等超参有关，这里就不讨论了。</p><p>不过，loss从0开始还是引起了我的好奇，让我们来探讨一下。</p><h2 id=grpo-loss函数分析>GRPO Loss函数分析</h2><p>GRPO（Group Relative Policy Optimization）损失函数：</p><p><img src=/images/grpo_loss.png alt="grpo loss"></p><p><a href=https://huggingface.co/docs/trl/grpo_trainer>GRPO Trainer</a></p><p>可以发现，grpo的loss可近似看作：</p><p>[ \text{Loss} \approx \frac{\pi_\theta}{\pi_\theta} \cdot A - \beta \cdot \text{KL} ]</p><p>其中，(\pi_\theta)表示当前策略，(A)表示奖励函数，(\beta)是KL散度的权重。</p><p>这里πθ/πθ不是1吗？让我们看看代码具体是怎么写的吧。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># x - x.detach() allows for preserving gradients from x</span>
</span></span><span class=line><span class=cl><span class=n>per_token_loss</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>per_token_logps</span> <span class=o>-</span> <span class=n>per_token_logps</span><span class=o>.</span><span class=n>detach</span><span class=p>())</span> <span class=o>*</span> <span class=n>advantages</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>per_token_loss</span> <span class=o>=</span> <span class=o>-</span><span class=p>(</span><span class=n>per_token_loss</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>per_token_kl</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=p>((</span><span class=n>per_token_loss</span> <span class=o>*</span> <span class=n>completion_mask</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>completion_mask</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>由于.detach()只是返回一个共享存储位置但没有梯度的tensor，所以<code>per_token_logps - per_token_logps.detach()</code>为0，<code>torch.exp(per_token_logps - per_token_logps.detach())</code>等于1，因此，此时的<code>per_token_loss</code>等于<code>advantages</code>。</p><p>只不过如果计算这一步的梯度的话，<code>per_token_logps.detach()</code>就要被看做常数C了，所以整体是有<code>per_token_logps</code>梯度的。</p><p>接下来的就跟论文里的Loss差不多了，所以要看第一步的loss，就要分别看KL和advantages（也就是reward）</p><p>（爆论在下一大章节的最后，请一定要看）</p><h2 id=初始loss为零的原因>初始Loss为零的原因</h2><h3 id=第一步的kl为什么是0>第一步的KL为什么是0？</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>unwrap_model_for_generation</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>accelerator</span><span class=p>)</span> <span class=k>as</span> <span class=n>unwrapped_model</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt_completion_ids</span> <span class=o>=</span> <span class=n>unwrapped_model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>prompt_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=n>prompt_mask</span><span class=p>,</span> <span class=n>generation_config</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>generation_config</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ref_per_token_logps</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_get_per_token_logps</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>ref_model</span><span class=p>,</span> <span class=n>prompt_completion_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>logits_to_keep</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span><span class=line><span class=cl><span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>prompt_ids</span><span class=p>,</span> <span class=n>completion_ids</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>per_token_logps</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_get_per_token_logps</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>logits_to_keep</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Compute the KL divergence between the model and the reference model</span>
</span></span><span class=line><span class=cl><span class=n>ref_per_token_logps</span> <span class=o>=</span> <span class=n>inputs</span><span class=p>[</span><span class=s2>&#34;ref_per_token_logps&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>per_token_kl</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>ref_per_token_logps</span> <span class=o>-</span> <span class=n>per_token_logps</span><span class=p>)</span> <span class=o>-</span> <span class=p>(</span><span class=n>ref_per_token_logps</span> <span class=o>-</span> <span class=n>per_token_logps</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span>
</span></span></code></pre></td></tr></table></div></div><p>由于grpo在这边的原理是：</p><ol><li><p>先由被训练模型（actor模型）推理生成prompt_completion_ids；</p></li><li><p>再把prompt_completion_ids给参考模型ref_model，生成ref_per_token_logps ；</p></li><li><p>把prompt_completion_ids给actor模型，拿到per_token_logps ；</p></li><li><p>最后KL = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1</p></li></ol><p>而在第一步的时候，actor模型此时权重与参考模型ref_model一致，所以per_token_logps = ref_per_token_logps ，代入公式中，所以KL=0 .</p><h3 id=第一步的advantages为什么是0>第一步的advantages为什么是0？</h3><p>这边需要我们再看一下上文的GRPO Loss是怎么算的：</p><ol><li><p>首先按照分组Group，对组内各样本（一个问题prompt生成num_generations个回答）进行标准化；</p></li><li><p>在最后计算loss时进行累加求均值的操作。</p></li></ol><p>而具体的代码实现则是：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Sum the rewards from all reward functions</span>
</span></span><span class=line><span class=cl><span class=n>rewards</span> <span class=o>=</span> <span class=n>rewards_per_func</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Compute grouped-wise rewards</span>
</span></span><span class=line><span class=cl><span class=n>mean_grouped_rewards</span> <span class=o>=</span> <span class=n>rewards</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_generations</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>std_grouped_rewards</span> <span class=o>=</span> <span class=n>rewards</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_generations</span><span class=p>)</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Normalize the rewards to compute the advantages</span>
</span></span><span class=line><span class=cl><span class=n>mean_grouped_rewards</span> <span class=o>=</span> <span class=n>mean_grouped_rewards</span><span class=o>.</span><span class=n>repeat_interleave</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_generations</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>std_grouped_rewards</span> <span class=o>=</span> <span class=n>std_grouped_rewards</span><span class=o>.</span><span class=n>repeat_interleave</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_generations</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>advantages</span> <span class=o>=</span> <span class=p>(</span><span class=n>rewards</span> <span class=o>-</span> <span class=n>mean_grouped_rewards</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>std_grouped_rewards</span> <span class=o>+</span> <span class=mf>1e-4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Slice to keep only the local part of the data</span>
</span></span><span class=line><span class=cl><span class=n>process_slice</span> <span class=o>=</span> <span class=nb>slice</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>accelerator</span><span class=o>.</span><span class=n>process_index</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>prompts</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>accelerator</span><span class=o>.</span><span class=n>process_index</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>prompts</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>advantages</span> <span class=o>=</span> <span class=n>advantages</span><span class=p>[</span><span class=n>process_slice</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># x - x.detach() allows for preserving gradients from x</span>
</span></span><span class=line><span class=cl><span class=n>advantages</span> <span class=o>=</span> <span class=n>inputs</span><span class=p>[</span><span class=s2>&#34;advantages&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>per_token_loss</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>per_token_logps</span> <span class=o>-</span> <span class=n>per_token_logps</span><span class=o>.</span><span class=n>detach</span><span class=p>())</span> <span class=o>*</span> <span class=n>advantages</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>per_token_loss</span> <span class=o>=</span> <span class=o>-</span><span class=p>(</span><span class=n>per_token_loss</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>beta</span> <span class=o>*</span> <span class=n>per_token_kl</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=p>((</span><span class=n>per_token_loss</span> <span class=o>*</span> <span class=n>completion_mask</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>completion_mask</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><ol><li><p>micro_batch_size必须是num_generations（一个问题生成多少个回答）的整数倍，为了简单起见，这里我们可以假设micro_batch_size=num_generations。</p></li><li><p>将每个子reward函数的值求和，得到一条样本的reward；</p></li><li><p>按照分组，算出num_generations个样本的mean_grouped_rewards 和std_grouped_rewards ；</p></li><li><p>将reward进行标准化，得到分组的advantages 。（按照进程切片，就是为了得到该进程（卡）上的分组advantages）</p></li><li><p>由于在上文已知，第一步的KL=0，所以此时的per_token_loss =advantages。再执行.sum(dim=1)，即将组内的advantages求和。再执行.mean()，即得到了组间的advantages均值，即原始输入的问题个数的均值。</p></li></ol><p>在这里我们可以注意到，第4步是对组内每个样本的reward进行标准化，第5步时对组内的标准化后的reward求和。那么<strong>对于标准化公式(ri - mean) / std 求和，就正好分子为0了</strong>。</p><p>换言之，其实<strong>GRPO Loss就等于βKL</strong>。只不过advantages可以在梯度计算中保留（见上文的.detach()）。</p><p><img src=/images/grpo_kl.png alt="grpo kl"></p><p>而上文说到，第一步的KL是0，所以<strong>第一步的loss一定是0</strong>.</p><h2 id=有啥改进措施加速方法>有啥改进措施（加速方法）？</h2><h3 id=loss是0为啥还能训>loss是0为啥还能训？</h3><p>哈，用loss计算梯度，loss为0不代表梯度也为0。</p><p>而 梯度 * 学习率 才是模型能训练的原因。</p><h3 id=改进措施>改进措施</h3><p>所以按照上文的思路，其实一开始100步几乎训不动，其实是因为学习率太小的缘故。</p><p>所以我们只需要关闭warmup，就可以从第一步开始训啦。</p></div><div class=post-reward><input type=checkbox name=reward id=reward hidden>
<label class=reward-button for=reward>赞赏支持</label><div class=qr-code><label class=qr-code-image for=reward><img class=image src=/images/money_weixin_20200719212002.png>
<span>微信打赏</span></label>
<label class=qr-code-image for=reward><img class=image src=/images/alipay_20200801211208.png>
<span>支付宝打赏</span></label></div></div><footer class=post-footer><div class=post-tags><a href=/tags/ai/>ai</a>
<a href=/tags/llm/>llm</a>
<a href=/tags/grpo/>grpo</a>
<a href=/tags/loss/>loss</a></div><nav class=post-nav><a class=prev href=/post/grpo_repetition/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">R1-Zero复现</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ai_agent/><span class="next-text nav-default">AI_Agent让大模型使用工具</span>
<span class="next-text nav-mobile">下一篇</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js crossorigin=anonymous></script>
<script type=text/javascript>var gitalk=new Gitalk({id:"2025-02-11 23:54:58 \u002b0800 \u002b0800",title:"GRPO Loss初期为0的原因与改进方法",clientID:"5ea75f603117948d8d37",clientSecret:"26c617c6bce9a975c2a65a68f1ca2a2cc7dde587",repo:"blog",owner:"zhangsheng377",admin:["zhangsheng377"],body:decodeURI(location.href),proxy:"https://vercel.prohibitorum.top/github_access_token"});gitalk.render("gitalk-container")</script><noscript>Please enable JavaScript to view the <a href=https://github.com/gitalk/gitalk>comments powered by gitalk.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:435878393@qq.com class="iconfont icon-email" title=email></a>
<a href=https://www.linkedin.com/in/zhangshengdong/ class="iconfont icon-linkedin" title=linkedin></a>
<a href=https://github.com/zhangsheng377/ class="iconfont icon-github" title=github></a>
<a href=https://www.zhangshengdong.com/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>由 <a class=hexo-link href=https://gohugo.io>Hugo</a> 强力驱动</span>
<span class=division>|</span>
<span class=theme-info>主题 -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span><div class=busuanzi-footer><span id=busuanzi_container_site_pv>本站总访问量 <span id=busuanzi_value_site_pv><img src=/img/spinner.svg alt=spinner.svg></span> 次</span>
<span class=division>|</span>
<span id=busuanzi_container_site_uv>本站总访客数 <span id=busuanzi_value_site_uv><img src=/img/spinner.svg alt=spinner.svg></span> 人</span></div><span class=copyright-year>&copy;
2019 -
2025<span class=heart><i class="iconfont icon-heart"></i></span><span><a href=https://beian.miit.gov.cn/ target=_blank>苏ICP备15009593号-1</a></span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/raphael@2.2.7/raphael.min.js integrity="sha256-67By+NpOtm9ka1R6xpUefeGOY8kWWHHRAKlvaTJ7ONI=" crossorigin=anonymous></script> <script src=https://cdn.jsdelivr.net/npm/flowchart.js@1.8.0/release/flowchart.min.js integrity="sha256-zNGWjubXoY6rb5MnmpBNefO0RgoVYfle9p0tvOQM+6k=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.js integrity="sha256-4O4pS1SH31ZqrSO2A/2QJTVjTPqVe+jnYgOWUVr7EEc=" crossorigin=anonymous></script> <script src=https://cdn.jsdelivr.net/npm/snapsvg@0.5.1/dist/snap.svg-min.js integrity="sha256-oI+elz+sIm+jpn8F/qEspKoKveTc5uKeFHNNVexe6d8=" crossorigin=anonymous></script> <script src=https://cdn.jsdelivr.net/npm/underscore@1.8.3/underscore-min.js integrity="sha256-obZACiHd7gkOk9iIL/pimWMTJ4W/pBsKu+oZnSeBIek=" crossorigin=anonymous></script> <script src=https://cdn.jsdelivr.net/gh/bramp/js-sequence-diagrams@2.0.1/dist/sequence-diagram-min.js integrity=sha384-8748Vn52gHJYJI0XEuPB2QlPVNUkJlJn9tHqKec6J3q2r9l8fvRxrgn/E5ZHV0sP crossorigin=anonymous></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/gh/bramp/js-sequence-diagrams@2.0.1/dist/sequence-diagram-min.css integrity=sha384-6QbLKJMz5dS3adWSeINZe74uSydBGFbnzaAYmp+tKyq60S7H2p6V7g1TysM5lAaF crossorigin=anonymous><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script>
<script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script>
<script id=baidu_analytics>var _hmt=_hmt||[];(function(){if(window.location.hostname==="localhost")return;var t,e=document.createElement("script");e.async=!0,e.src="https://hm.baidu.com/hm.js?c602db2501643f661d9789f9e9707386",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script></body></html>