---
title: "树模型"
date: 2021-03-16T02:09:58+08:00
lastmod: 2021-03-16T02:09:58+08:00
draft: false
keywords: []
description: ""
tags: [树模型, 决策树, ID3, C4.5, CART,  AdaBoost]
categories: []
author: ""

# You can also close(false) or open(true) something for this content.
# P.S. comment can only be closed
comment: false
toc: true
autoCollapseToc: false
postMetaInFooter: false
hiddenFromHomePage: false
# You can also define another contentCopyright. e.g. contentCopyright: "This is another copyright."
contentCopyright: false
reward: false
mathjax: false
mathjaxEnableSingleDollar: false
mathjaxEnableAutoNumber: false

# You unlisted posts you might want not want the header or footer to show
hideHeaderAndFooter: false

# You can enable or disable out-of-date content warning for individual post.
# Comment this out to use the global config.
#enableOutdatedInfoWarning: false

flowchartDiagrams:
  enable: false
  options: ""

sequenceDiagrams: 
  enable: false
  options: ""

---

# 树模型

在各种机器学习的算法中，树模型可以说是最贴近于人类思维的模型，一切的树模型都是一种基于特征空间划分的具有树形分支结构的模型。举个直观的例子：决策树。

## 决策树

决策树可以说是树模型中最基础，也是最有名的模型，它可以被认为是一堆if-then(条件判断)的规则集合。

![决策树](/images/c2cec3fdfc0392456a6ac4258694a4c27d1e2538.png)

比如这张图，人们该如何去判断是否应该出去玩。

1. 首先，我们可以看天气预报，如果是多云，那我们就可以出去玩；

2. 但如果是晴天，那我们还需要考虑湿度，如果湿度小于等于70%，也就是人体感觉舒适的湿度，那我们就可以出去玩，反之，则不行；

3. 那么另一种情况，下雨天，我们还需要去看是否在刮大风，如果已经起风了，则说明马上就要下雨了，我们也不能出去玩；但是如果还没起风，我们还是可以出去试试运气的，兴许是天气预报错了呢~

   

看到这里，大家可能会觉得，决策树模型这么简单呀，就是人类的正常思维嘛。的确，决策树模型的原理就是这么的简单，但还有几个问题我们需要上升到理论的高度去解决：

* 上图，人类在推理的时候，所看到的数据特征其实有 天气预报，湿度，还有是否刮风等特征，那么我们该选择哪个特征去划分特征空间，即建立节点呢？这就是`特征选择`和`决策树生成`的问题。
* 还有，我们通过递归去划分特征空间，从而建立决策树的方式，很可能会导致过拟合的问题。一般来说，这就需要我们在建立好树之后，对已生成的树做自下而上的剪枝。那么如何剪枝，也是一个我们需要思考的问题。

   

### 特征选择及决策树生成

那么，我们首先来看特征选择问题。

如何选择一个特征进行划分，其实相当于是在讨论，用哪一个特征去划分特征空间，可以最有利于我们的分类，换言之，也就是降低了子特征空间的无序程度，也就是熵。

那么，熵既然表示的是无序的程度，它就肯定是和变量的分布有关系的。于是，我们假设某个特征$X$为$x_i$时的概率，为$p_i$：

$$
P(X=x_i) = p_i
$$

于是，我们就可以把特征$X$的熵定义为：

$$
H(X) = -\sum_{i=1}^n p_i log p_i
$$

所以，到此为止，一个给定数据集的熵我们就可以计算出来了。但是，我们选取一个特征来划分数据集，所要比较的其实是划分前后熵的变化，所以，我们就可以定义$ 信息增益 $为数据集$D$的熵，与 在给定特征$A$的条件下数据集D的熵之差，即：

$$
g(D,A) = H(D) - H(D|A)
$$

所以，我们最终要选取的特征，也就是给定哪个特征$A$，它的信息增益最大，那它就是我们要用来划分数据集的那个特征。



#### ID3算法

刚才，我们介绍了特征选择的大体思路。那么接下来，我们来介绍一下具体的算法实现。

ID3算法，其实就是我们刚才所说的，每次选取信息增益最大的特征进行划分，递归地执行上述的步骤，直到所有特征的信息增益很小或者没有特征了为止。



#### C4.5算法

而我们刚才说的信息增益，其实有个小问题，就是它会偏向于去选择种类较多的那个特征。比如，一个特征里所有的取值都是唯一的，那么一旦选择这个特征进行划分，则可以在子特征空间中一下减少该特征一半的种类，但其实子特征空间里的该特征依旧全部是唯一的取值，即无序程度并没有减少。所以，C4.5算法就使用信息增益比，来校正这个问题。

$$
g_R(D,A) = {g(D,A)\over H_A(D)}
$$

即，信息增益比$g_R(D,A)$，为其信息增益$g(D,A)$与数据集D关于特征A的熵$H_A(D)$ 之比。

而其余建立树的步骤，C4.5算法和ID3算法一样。



#### CART算法

分类与回归树(classification and regression tree, CART)模型，该模型假设决策树是二叉树，其内部节点相当于是在做一个二元的是否的判断。

CART算法是对回归树使用平方误差最小化准则，而对分类树使用基尼指数最小化准则，来进行特征选择，从而生成二叉树的。
$$
Gini(p) = \sum_{k=1}^Kp_k(1-p_k) = 1 - \sum_{k=1}^K(p_k)^2
$$

而基尼指数$Gini(D)$是用来表示样本集合D的不确定性。所以当选择基尼指数进行特征划分时，其实是在使划分之后的子特征空间的不确定性，在逐步的减小。



### 决策树的剪枝

那么接下来我们来讲一下决策树的剪枝。

我们之前说到过，建立决策树的时候，我们是采用递归的方法，一直建立到不能继续了为止。但是，这样产生的树往往泛化能力比较弱。所以，我们需要考虑树的复杂度，对已生成的决策树进行简化，从而提高泛化能力。

$$
C_\alpha(T) = \sum_{t=1}^{|T|}N_tH_t(T) + \alpha|T|
$$

这个就是决策树的损失函数$C_\alpha(T)$，其中，$t$为决策树$T$的叶子节点，$N_t$为该叶子节点的样本个数，而$H_t(T)$为叶子节点$t$的熵，最后的$\alpha$则是正则化系数。

所以，在有了这个式子之后，我们就可以来讲一下剪枝的思路了：

首先，我们计算出每个节点的熵，然后递归的从叶节点向上尝试回缩，从而计算回缩前后树的损失函数；

那么我们最后所要得到的，就是损失函数最小的那棵决策树了。





## 集成学习

但是，光有一个决策树模型去解决问题，可能还不够好。所以，我们就需要一种可以去集合多个模型来共同解决问题的方式。也就是这个集成学习。

集成学习(ensemble learning) 是通过构建并结合多个学习器，来完成任务的一种方法。

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/ensembling.png)

而它的理论基础呢，是Schapire这个人在1995年，证明了 强可学习与弱可学习是等价的，这么一个概念。所谓弱可学习就是，这个模型的正确率只比随机猜测要好一点点。而强可学习呢，就是指这个模型的效果会非常的好。

![img](https://pic1.zhimg.com/80/v2-6dfc944cd6c241d5fb47405173d957fc_720w.jpg)

但是，按照我们一般的感觉，好像如果简单地把一堆学习器相结合，也就是相加求平均，那么它的效果应该是比最差的要好一点，但是比最好的要差一点。所以，我们应该怎样才能得到比最好的学习器还要好的结合效果呢？

这个答案其实是，我们的每个个体学习器要“好而不同”。也就是说，每个学习器都不能太差，并且要有所差异，才能综合出一个更好的结果。

那么，我们应该如何进行综合呢？

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/single-bagging-boosting.png)

我们按照个体学习器之间是否存在依赖关系，可以分为两种：

1. 第一种是个体学习器之间不存在强依赖关系，它们可以并行的去生成。这一类的代表方法是bagging和随机森林算法。
2. 而第二种则是个体学习器之间存在强依赖，即后一个学习器的构建需要依赖前一个学习器的结果，它们是串行构建的。这种类别的代表方法是boosting算法。



### bagging

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/bagging.jpg)

而前面所说的，可以进行并行构建的bagging算法，它的主要思想其实是，对训练集进行随机抽样，从而用不同的子训练集去并行的生成各个基模型。

还有像随机森林算法，它也可以算作是bagging的一个变种。随机森林其实就是我们之前所提到的决策树，只不过每次划分时并不是选取最优的那个特征，而是先随机选取k个特征，然后从这k个特征中选择最优的那个特征进行划分。



### 提升方法(boosting)

而这个boosting方法呢，它是去改变训练数据的概率或权重分布，从而针对不同的训练数据，训练出一系列的弱分类器。

![img](https://www.biaodianfu.com/wp-content/uploads/2020/09/boosting.jpg)

也就相当于是，每个基模型，都在用之前模型的残差，对数据进行抽样。而残差就是观察值与预测值之间的差。

一般来说，在这里，残差就等于误差。而
$$
误差=偏差+方差+噪声
$$
![img](https://images2018.cnblogs.com/blog/938105/201806/938105-20180628190351688-673987402.jpg)

偏差就是指预测值与真实值的平均差。方差则是预测值与预测的平均值之间的差。如上图所示，当它偏差比较小时，它的预测值的中心点就离真实值比较近；而方差则说明了预测值的离散程度。

那么大家可以想哦，我们上一张所说的bagging算法，它是随机抽取的样本，所以每个它的样本集的分布都是近似的，所以bagging算法并不能降低偏差，但是它的模型是有差异的且独立的，所以bagging算法可以在一定程度上降低方差。

而boosting算法其实是在不断地拟合残差，所以它可以降低偏差；但是由于它的基模型之间是串行的，也就是说boosting的基模型之间是强相关的，所以，它并不能明显的降低方差。想要方差小则需要在目标函数中添加正则项，防止过拟合。

好，那么接下来我们来讲一个著名的，boosting的实现方法。

#### AdaBoost提升方法

也就是这个，AdaBoost (Adaptive Boosting)提升方法，它是比较具有代表性的一种提升方法。它具体的做法是，提高那些被前一轮弱分类器错误分类的样本的权重，而降低那些已经被正确分类的样本的权重。那么这样一来，在上一轮分类器中没有被正确分类的样本，就可以在这一轮的弱分类器中，得到更大的关注。

而最终，AdaBoost会采取加权投票的方式，去综合多个弱分类器的结果。



#### 提升树

而提升树(boosting tree)算法，则是以决策树(分类树或回归树)为基分类器的提升方法，可以算作是AdaBoost算法的特殊情况。



#### 梯度提升算法

我们前面提到过，其实boosting算法就是在不断地拟合残差。但是，当残差作为目标的时候，往往并不是那么容易去优化。

所以，Freidman就提出了梯度提升(gradient boosting)算法。它主要思想是把残差作为我们的损失函数，所以我们的优化方法就可以去计算损失函数的梯度，使其沿着负梯度的方向前进，也就是梯度下降法。

反映到boosting算法中，也就相当于是，用损失函数的负梯度，来作为残差的近似值，去改变错误分类的权重。说白了就是相当于用梯度信息进行加权。

而在梯度提升算法Gradient Boosting中，最有代表性的就是GBDT(Gradient Boosting Descision Tree) 梯度提升决策树。

这个名字大家其实可以发现哦，其实这个GBDT，它里面包含了DT(Descision Tree)决策树、Boosting 提升方法(也就是用一组弱分类器综合成一个强分类器)，和 Gradient Boosting 梯度提升方法(也就是用梯度信息对弱分类器进行加权)，而梯度提升方法，其实就是用计算梯度的方式，去实现这个Boosting 提升方法。



## xgboost

接下来就到了我们今天介绍的最后一个算法，XGBoost(Extreme Gradient Boosting)，它其实是GBDT的一种高效实现。

那么，xgb与gbdt主要的差别在于：

1. 传统的GBDT在计算loss函数时，只使用了一阶导数，而XGBoost对Loss进行泰勒展开，取了一阶导数和二阶导数。这样就可以提高xgb的准确率。
2. 同时，XGBoost还考虑了正则化项，包含了对复杂模型的惩罚，比如叶节点的个数、树的深度这些。那么这样就可以确保树的简单性，也就提高了泛化程度。
3. 还有采用列抽样和并行处理等方式，提高了xgb的效率。

https://zhuanlan.zhihu.com/p/83901304



## 总结及与业务的结合

好，最后总结一下就是，树模型的本质其实是去改变样本集的权重或是概率的分布，从而将许多个体学习器的结果综合起来。那么，从这一过程我们其实可以看出，树模型其实更加适合于去处理异质化数据。什么是异质化数据呢？就是说比如数据不光有embedding这个特征，还有段落数、字数、图片个数等其他含义的特征。那么这种数据使用树模型时，往往就可以对数据特征关注得比较充分，模型效果也就比较好了。比方说我们的负面和优质模型。

这些模型，特别是优质模型，怎么去判断一篇文章的优质，它不光关注文章的语义信息(具体写的是什么)，还要更加关注文章的结构信息(有几段，有多少图片等行文布局方面的特征)，所以我们就可以把这些异质的特征，做好处理后，采用诸如xgboost、catboost这类树模型去解决问题。事实上，在我们的优质模型没有上线之前，运营要人工为每日精选栏目去挑选优质文章，这样挑选的数目少、人还累，而自从模型上线之后，就变为了由模型直接入库和人工审核的模式，节约了大量人力，可用的文章数目还多，获得了一致的好评。

最后再说一点，其实对于同质数据的处理，比如像是分类模型，它就比较关注文章的语义信息，所以只要文章embedding就够了。那么我们现在所在研究的一个方向，其实就是用多任务、多输出的深度模型，比如底层我们采用bert做一个共享参数的embedding层，然后上面连接3个用于输出的子网络层，分别学习一、二、三级的分类信息。这种树状多任务模型结构的好处是，由于底层的参数被所有目标共享，所以大大降低了过拟合的风险；与此同时，不同目标在学习时也可以通过这些共享的参数进行知识迁移，从而利用其它目标学习到的知识帮助自己目标的学习，起到一个纠偏的作用。在我们实际的实验中，这种多任务模型，与之前每层训练一个单任务的模型，相比，它的指标是有很大提升的。所以这个也是未来的一个演进方向。

![img](https://pic2.zhimg.com/80/v2-b03e69ef4c1374d2874721b5a3181299_720w.jpg)









