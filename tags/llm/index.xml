<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>llm on 张胜东的博客</title><link>https://www.zhangshengdong.com/tags/llm/</link><description>Recent content in llm on 张胜东的博客</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>&lt;a href="https://beian.miit.gov.cn/" target="_blank">苏ICP备15009593号-1&lt;/a></copyright><lastBuildDate>Wed, 05 Mar 2025 23:54:58 +0800</lastBuildDate><atom:link href="https://www.zhangshengdong.com/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>R1-Zero复现</title><link>https://www.zhangshengdong.com/post/grpo_repetition/</link><pubDate>Wed, 05 Mar 2025 23:54:58 +0800</pubDate><guid>https://www.zhangshengdong.com/post/grpo_repetition/</guid><description>前言 最近大伙儿们对R1的复现热情可谓是空前高涨，我也趁此机会，用家里的单卡3090，使用openr1+lora，尝试复现一把openr1-z</description></item><item><title>GRPO Loss初期为0的原因与改进方法</title><link>https://www.zhangshengdong.com/post/grpo_loss/</link><pubDate>Tue, 11 Feb 2025 23:54:58 +0800</pubDate><guid>https://www.zhangshengdong.com/post/grpo_loss/</guid><description>引言 在家里自己用OpenR1准备从qwen-base训出个R1模型来，结果跑了demo数据，发现前100多步的loss几乎都是0： 在搜索相关</description></item><item><title>AI_Agent让大模型使用工具</title><link>https://www.zhangshengdong.com/post/ai_agent/</link><pubDate>Sun, 18 Aug 2024 23:54:58 +0800</pubDate><guid>https://www.zhangshengdong.com/post/ai_agent/</guid><description>背景 近一年，ai_agent变得异常火热，从某个方面来说，rag也是agent的一个tool而已，所以我们大胆的预判：未来是属于agent的</description></item><item><title>大模型检索增强生成RAG</title><link>https://www.zhangshengdong.com/post/rag/</link><pubDate>Sat, 06 Apr 2024 01:41:58 +0800</pubDate><guid>https://www.zhangshengdong.com/post/rag/</guid><description>背景 最近LLM大模型异常火热，我判断RAG检索增强是未来的一个重要切入点，所以想试试做个demo，走一遍流程。 主要的想法是利用我的微信公众号</description></item></channel></rss>